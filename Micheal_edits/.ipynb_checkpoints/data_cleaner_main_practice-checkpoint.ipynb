{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dependent libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import panel as pn\n",
    "pn.extension('plotly')\n",
    "from panel.interact import interact\n",
    "from panel import widgets\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv #Just in case we need an API key.\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "#Import Various Libraries, including Tweepy, a Python library for the Twitter API.\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from textblob import TextBlob \n",
    "#%matplotlib inline\n",
    "import tweepy\n",
    "\n",
    "load_dotenv()\n",
    "pd.options.display.float_format = '{:.2f}'.format\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass API Keys to Twitter API and build Tweepy API handler object.\n",
    "consumer_key = os.getenv('ALT_TWITTER_API_KEY')\n",
    "consumer_secret = os.getenv('ALT_TWITTER_SECRET_KEY')\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a DataFrame for tweet search results data structure.\n",
    "dict_tweet_structure = {\"twitter_user\":\"\",\"category\":[],\"time\":\"\",\"sentiment\":\"\",\"text\":\"\",\"tweet_id\":\"\",\n",
    "                        \"tweet_source\":\"\",\"quote_count\":0,\"reply_count\":0,\"retweet_count\":0,\"fav_count\":0,\n",
    "                        \"Polarity Rating\": \"\", \"Popularity Rating\":\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing primary DataFrame of tweets. DON'T RUN! Or you will lose your data. :)\n",
    "df_tweets_found = pd.DataFrame(dict_tweet_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Search Term Library\n",
    "dict_search_terms = {'bitcoin':['bitcoin','btc','#bitcoin',],\n",
    "                     'generic':['cryptocurrency','blockchain'],\n",
    "                     'litecoin':['litecoin','ltc','#litecoin', '#ltc'],\n",
    "                     'ethereum':['ethereum','eth','#ethereum','#eth']   }\n",
    "#Define Twitter User List\n",
    "list_twitterers = ['joerogan','elonmusk','officialmcafee','vitalikbuterin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for creating search query string for tweepy.Cursor(api.search). Max query length is 128 for sandbox env.\n",
    "#In it's current form it supports a bunch of keywords joined by OR, grouped by (), AND from a single tweeter.\n",
    "def query_creator(list_keywords, twitter_user = \"!\", mood = \"!\"):\n",
    "    query = \"(\"\n",
    "    for keyword in (list_keywords):\n",
    "        query += keyword\n",
    "        if keyword != list_keywords[-1]:\n",
    "            query += \" OR \"\n",
    "        else:\n",
    "            query += \")\"\n",
    "    if twitter_user != \"!\":\n",
    "        query += \" from:\" + twitter_user\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the core function for tweet searching.\n",
    "def tweet_search_full(string_query, date_from = \"200603210000\", date_to = \"201801010000\", number_tweets = 500):\n",
    "    for tweets in tweepy.Cursor(api.search_full_archive, query=string_query, environment_name='CryptoSentimentQueryProd',\n",
    "                                fromDate=date_from, toDate=date_to).items():\n",
    "        #Primary environment name = CryptoSentimentFullArc\n",
    "        screen_name = tweets.user.screen_name\n",
    "        contents = \"\"\n",
    "        #if tweets.truncated == True:\n",
    "        #    contents = tweets.full_text\n",
    "        #else:\n",
    "        contents = tweets.text\n",
    "        category = category_key(contents,dict_search_terms)\n",
    "        yield [screen_name, category, str(tweets.created_at), \"\", contents, tweets.id, tweets.source, tweets.quote_count,\n",
    "               tweets.reply_count, tweets.retweet_count, tweets.favorite_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function searches the last month, but uses the same arguments and formatting as full archive; useful for testing without using up queries.\n",
    "def tweet_search_month(string_query, date_from = \"200603210000\", date_to = \"202101170000\", number_tweets = 500):\n",
    "    for tweets in tweepy.Cursor(api.search_30_day, query=string_query, environment_name='CryptoSentimentQueryTest',\n",
    "                                fromDate=date_from, toDate=date_to,).items():\n",
    "        screen_name = tweets.user.screen_name\n",
    "        contents = \"\"\n",
    "        #if tweets.truncated == True:\n",
    "        #    contents = tweets.full_text\n",
    "        #else:\n",
    "        contents = tweets.text\n",
    "        \n",
    "        category = category_key(contents,dict_search_terms)\n",
    "        yield [screen_name, category, str(tweets.created_at), str(tweets.coordinates), contents, tweets.id, tweets.source, tweets.quote_count,\n",
    "               tweets.reply_count, tweets.retweet_count, tweets.favorite_count]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_search_free(string_query, number_tweets = 100):\n",
    "    for tweets in tweepy.Cursor(api.search, q=string_query).items(number_tweets):\n",
    "        screen_name = tweets.user.screen_name\n",
    "        contents = \"\"\n",
    "        #if tweets.truncated == True:\n",
    "        #    contents = tweets.full_text\n",
    "        #else:\n",
    "        contents = tweets.text\n",
    "        polarity = sentiment_reader(contents)\n",
    "        \n",
    "        category = category_key(contents,dict_search_terms)\n",
    "        yield [screen_name, category, str(tweets.created_at), str(tweets.coordinates), contents, tweets.id, tweets.source,\"x\",\n",
    "               \"x\", tweets.retweet_count, tweets.favorite_count, polarity, (polarity * int(tweets.retweet_count))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for appending search results to the tweet df.\n",
    "def tweet_dataframe_append(rows, target_dataframe):\n",
    "    for row in rows:\n",
    "        series_result = pd.Series(row, index=target_dataframe.columns)\n",
    "        target_dataframe = target_dataframe.append(series_result, ignore_index=True)\n",
    "        return target_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for generating a list of categories from the presence of keywords in text.\n",
    "def category_key(text_block,dict_keywords):\n",
    "    category_list = []\n",
    "    output = \" \"\n",
    "    contents = text_block.lower()\n",
    "    for key in dict_keywords:\n",
    "        for keyword in dict_keywords[key]:\n",
    "            if keyword in contents:\n",
    "                category_list.append(key)\n",
    "    #return output.join(category_list)\n",
    "    return category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for generating sentiment values using textblob library.\n",
    "def sentiment_reader(text_block):\n",
    "    sentiment_text = TextBlob(text_block)\n",
    "    return sentiment_text.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for fixing duplicate values in the category column.\n",
    "def no_dupes(string):\n",
    "    string =  str(string)\n",
    "    if string[-1] != \" \":\n",
    "        string = string + \" \"\n",
    "    wordlist = []\n",
    "    output = \"\"\n",
    "    word = \"\"\n",
    "    for letter in string:\n",
    "        if letter != \" \":\n",
    "            word = word + letter\n",
    "        elif word not in wordlist:\n",
    "            wordlist.append(word)\n",
    "            output = output + word + \" \"\n",
    "            word = \"\"\n",
    "        else:\n",
    "            word = \"\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Query Creator\n",
    "query = query_creator(dict_search_terms['generic']+dict_search_terms['bitcoin']+dict_search_terms['litecoin'], 'satoshilite')\n",
    "print(query)\n",
    "print(len(query))\n",
    "#Query length is limited to 128 characters, max tweets per query is limited to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing primary search and DataFrame append.\n",
    "search_results = tweet_search_full(query)\n",
    "\n",
    "for tweets in search_results:\n",
    "    print (tweets)\n",
    "    series_result = pd.Series(tweets, index=df_tweets_found.columns)\n",
    "    df_tweets_found = df_tweets_found.append(series_result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_tweets_found' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-425-7c4685daf850>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_tweets_found\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/raw_data/raw_tweets_01.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_tweets_found' is not defined"
     ]
    }
   ],
   "source": [
    "df_tweets_found.to_csv('../data/raw_data/raw_tweets_01.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaning = pd.read_csv('../data/raw_data/raw_tweets_01_filter.csv')\n",
    "#df_cleaning.drop_duplicates(inplace=True)\n",
    "#df_cleaning.xs('text', axis=1)\n",
    "df_cleaning.loc[13]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run several times to eliminate usernames containing crypto.\n",
    "df_cleaning = df_cleaning[~df_cleaning.text.str.contains(\"_crypto\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaning.to_csv('../data/raw_data/raw_tweets_01_filter.csv',index=False)\n",
    "df_tweets_found\n",
    "#df_cleaning.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##trying to test sentiment analysis \n",
    "df_tweets_found = pd.read_csv('../data/raw_data/raw_tweets_01_filter.csv')\n",
    "\n",
    "#pull the tweet text as a list from the data frame\n",
    "tweet_text_list = df_tweets_found.text.tolist()\n",
    "\n",
    "\n",
    "#create objects for sentiment\n",
    "\n",
    "sentiment_text = [TextBlob(tweet) for tweet in tweet_text_list]\n",
    "\n",
    "sentiment_text[0].polarity , sentiment_text[0]\n",
    "\n",
    "#create sentiment list\n",
    "\n",
    "sentiment_list = [[tweet.sentiment.polarity, str(tweet)] for tweet in sentiment_text]\n",
    "sentiment_list[0]\n",
    "#remove the tweet from the list of lists\n",
    "for x in sentiment_list:\n",
    "    del x[1]\n",
    "# convert the list of lists into a DF column\n",
    "df_polarity = pd.DataFrame(sentiment_list, columns = ['Polarity'])\n",
    "#convert back to a single list\n",
    "polarity_list = df_polarity[\"Polarity\"].tolist()\n",
    "#append to OG data frame\n",
    "\n",
    "df_tweets_found['Polarity Rating'] = polarity_list\n",
    "\n",
    "df_tweets_found[\"Popularity Rating\"]= df_tweets_found[\"fav_count\"] * df_tweets_found[\"Polarity Rating\"]\n",
    "df_tweets_found.to_csv('../data/raw_data/raw_tweets_01_filter_polarity.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dict_tweet_structure' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-426-1bd4b609ed49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Using recent search to generate recent tweets from anyone that include our keywords.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_tweets_live\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_tweet_structure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_creator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_search_terms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'generic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdict_search_terms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bitcoin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdict_search_terms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'litecoin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdict_search_terms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ethereum'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlive_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet_search_free\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dict_tweet_structure' is not defined"
     ]
    }
   ],
   "source": [
    "#Using recent search to generate recent tweets from anyone that include our keywords.\n",
    "df_tweets_live = pd.DataFrame(dict_tweet_structure)\n",
    "query = query_creator(dict_search_terms['generic']+dict_search_terms['bitcoin']+dict_search_terms['litecoin']+dict_search_terms['ethereum'])\n",
    "print(query)\n",
    "live_results = tweet_search_free(query, 1000)\n",
    "for tweets in live_results:\n",
    "    #print (tweets)\n",
    "    df_tweets_live = tweet_dataframe_append(live_results, df_tweets_live)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "#df_tweets_live.hvplot()\n",
    "df_tweets_live.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_test = df_tweets_live#.filter(['time','Popularity Rating'] ,axis=1)#.hvplot(x='time',rot=90)\n",
    "polarity_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_test['time'] = pd.to_datetime(polarity_test['time'],format= '%M:%S',infer_datetime_format=True ).dt.time\n",
    "rolling_poll = polarity_test['Polarity Rating'].rolling(window=20).mean()\n",
    "end = pd.to_timedelta([str(polarity_test['time'].iloc[0])])\n",
    "start = pd.to_timedelta([str(polarity_test['time'].iloc[-1])])\n",
    "recent_tweets_duration = str((end-start)[0])\n",
    "roll = rolling_poll.hvplot(x='time',y='Polarity Rating',rot=90, hover_cols = ['twitter_user','text'])\n",
    "pol = polarity_test.hvplot(x='time',y='Polarity Rating',rot=90, hover_cols = ['twitter_user','text'],xticks=6,xlabel=f\"Duration length = {recent_tweets_duration}\")\n",
    "#print(polarity_test['Polarity Rating'].mean())\n",
    "#print((polarity_test['time'].iloc[-1]))\n",
    "#print((polarity_test['time'].iloc[0]))\n",
    "#polarity_test.mean()\n",
    "pol\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = pd.to_timedelta([str(polarity_test['time'].iloc[0])])\n",
    "start = pd.to_timedelta([str(polarity_test['time'].iloc[-1])])\n",
    "print ((end-start)[0])\n",
    "recent_tweets_duration = str((end-start)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graphing = pd.read_csv('../data/raw_data/raw_tweets_01_filter_polarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graphing = df_graphing.set_index('time')\n",
    "df_graphing.sort_index(inplace=True)\n",
    "df_graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graphing.drop('sentiment',axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graphing = df_graphing.reset_index()\n",
    "#pd.unique(df_graphing['category'])\n",
    "#df_bitcoin = df_graphing.groupby('category')\n",
    "#df_bitcoin.head()\n",
    "df_graphing['category'] = df_graphing.category.apply(no_dupes)\n",
    "#df_graphing.to_csv(\"../data/raw_data/raw_tweets_cleaning.csv\",index=False)\n",
    "df_graphing['time'] = pd.to_datetime(df_graphing['time'],format= '%b:%d:%Y',infer_datetime_format=True ).dt.date #\n",
    "#df_graphing\n",
    "df_graphing = df_graphing.set_index('time')\n",
    "df_graphing.sort_index(inplace=True)\n",
    "df_bitcoin = df_graphing.loc[df_graphing.category.str.contains('bitcoin')]\n",
    "df_ethereum = df_graphing.loc[df_graphing.category.str.contains('ethereum')]\n",
    "df_litecoin = df_graphing.loc[df_graphing.category.str.contains('litecoin')]\n",
    "df_generic = df_graphing.loc[df_graphing.category.str.contains('generic')]\n",
    "df_bitcoin_rolling = df_bitcoin['Polarity Rating'].rolling(window=50).mean()\n",
    "df_ethereum_rolling = df_ethereum['Polarity Rating'].rolling(window=50).mean()\n",
    "df_litecoin_rolling = df_litecoin['Polarity Rating'].rolling(window=50).mean()\n",
    "df_generic_rolling = df_generic['Polarity Rating'].rolling(window=50).mean()\n",
    "#df_bitcoin_rolling#.hvplot(rot=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graphing.to_csv(\"../data/raw_data/cleaner_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bitcoin_roller' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-427-b550d0668df4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbitcoin_roller\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0methereum_roller\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgeneric_roller\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlitecoin_roller\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bitcoin_roller' is not defined"
     ]
    }
   ],
   "source": [
    "bitcoin_roller * ethereum_roller * generic_roller * litecoin_roller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graphing.hvplot(groupby='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_bitcoin = df_bitcoin.hvplot(x='time',y='Popularity Rating',rot=90, hover_cols = ['twitter_user','text'])\n",
    "graph_ethereum = df_ethereum.hvplot(x='time',y='Popularity Rating',rot=90, hover_cols = ['twitter_user','text'])\n",
    "graph_litecoin = df_litecoin.hvplot(x='time',y='Popularity Rating',rot=90, hover_cols = ['twitter_user','text'])\n",
    "graph_generic = df_generic.hvplot(x='time',y='Popularity Rating',rot=90, hover_cols = ['twitter_user','text'])\n",
    "graph_etheruem + graph_bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the user_timeline method, which seems to return around 500~1000 tweets tops.\n",
    "ticker = 0\n",
    "for tweet in tweepy.Cursor(api.user_timeline, id='elonmusk',trim_user=True, max_id=2237531699681981416, count =10000, exclude_replies = True).items():\n",
    "    #if 'bitcoin' in tweet.text:\n",
    "    ticker += 1\n",
    "    print(tweet.text + \" \" + str(tweet.created_at)+\" \"+str(tweet.id)+\"\\n\")\n",
    "print(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweepy.Cursor(api.user_timeline, id=\"elonmusk\",tweet_mode=\"extended\").items(100):\n",
    "    #if 'Python' in tweet.full_text:\n",
    "        print(tweet.full_text)\n",
    "        #print(tweet.user['id_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defunct Old Search Method\n",
    "def tweet_gather(keywords, user_id):\n",
    "    #contents = \"\"\n",
    "    for tweet in tweepy.Cursor(api.user_timeline, id=user_id).items():\n",
    "        \"\"\"if tweet.truncated == True:\n",
    "            contents = tweet.full_text\n",
    "        else:\n",
    "            contents = str(tweet.text).lower()\"\"\"\n",
    "        \n",
    "        contents = tweet.text.lower()\n",
    "        #contents = contents.lower() \n",
    "        for word in keywords:           \n",
    "            if word.lower() in contents:\n",
    "                #print(tweet.text + \" \" + str(tweet.created_at)+\"\\n\")\n",
    "                \"\"\"Here we can append to a pd.DataFrame that collects the twitter handle, keywords used, keyword category\n",
    "                   for ex:'bitcoin' for hits on 'bitcoin' or 'btc', as well as any sentiment rating we create.\n",
    "                   Ulitmately we can then export that DataFrame to csv file.\"\"\"\n",
    "                yield tweet.text, str(tweet.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This is the basic api.search; it only returns tweets from the last 7 days.\n",
    "for tweet in tweepy.Cursor(api.search, q='(cryptocurrency OR crypto OR blockchain OR bitcoin OR struggle OR python)').items(1000):\n",
    "    print(tweet.text + \" \" + str(tweet.created_at)+\" \"+str(tweet.place)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for creating search query string for tweepy.Cursor(api.search_full_archive).\n",
    "def query_creator_full(list_keywords, twitter_user = \"!\", mood = \"!\"):\n",
    "    query = \" \"\n",
    "    query = query.join(list_keywords)\n",
    "    #print(query)\n",
    "    if twitter_user != \"!\":\n",
    "        query += \" from:\" + twitter_user\n",
    "        #print(query)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the data from Coinbase API\n",
    "- TODO\n",
    "    - Need to show data before dataframe conversion (show our process before creating function)\n",
    "    - Explain how we got column names\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_daily_data(symbol, start, end):\n",
    "    pair_split = symbol.split('/') # Splitting our symbol by the '/' and creating a a list for the new values.\n",
    "    symbol = pair_split[0] + '-' + pair_split[1] # symbol = BTC-USD #The API request format requires the dash.\n",
    "    url = f'https://api.pro.coinbase.com/products/{symbol}/candles?start={start}&end={end}&granularity=86400'#notice the symbol insert. There are 86400 seconds in a day.\n",
    "    response = requests.get(url) #getting response from website\n",
    "    if response.status_code == 200: # check to make sure the response from server is good\n",
    "        #if response is good then we create a dataframe by reformatting a json load.\n",
    "        data = pd.DataFrame(json.loads(response.text), columns=['unix', 'low', 'high', 'open', 'close', 'volume'])\n",
    "        data['date'] = pd.to_datetime(data['unix'], unit='s') # convert to a readable date\n",
    "       #######\n",
    "\n",
    "        # if we failed to get any data, print an error...otherwise write the file\n",
    "        if data is None:\n",
    "            print(\"Did not return any data from Coinbase for this symbol\")\n",
    "        else:\n",
    "            data.to_csv(f'Coinbase_{pair_split[0] + pair_split[1]}_dailydata_{end}.csv', index=False)\n",
    "    else:\n",
    "        print(\"Did not receieve OK response from Coinbase API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the fetch function with a function focused on our three main cryptos: BTC/USD, ETH/USD, LTC/USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "yesterday = today - timedelta(days = 1)\n",
    "yesterday = yesterday.strftime(\"%Y-%m-%d\")\n",
    "#start_date = yesterday - timedelta(days = 300)#wont let me get more then 298 days, or so.\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2015-09-30'\n",
    "#start_date = start_date.strftime(\"%Y-%m-%d\")\n",
    "#end_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "cryptolist = ['BTC/USD', 'ETH/USD', 'LTC/USD']\n",
    "\n",
    "#function to pull crypto data from coinbase api passing in crypto symbol pair, start and end date,\n",
    "def fetch_main_cryptos(crypto):\n",
    "    fetch_daily_data(crypto, start_date, end_date)\n",
    "\n",
    "    \n",
    "#call the function calling our API loop thrgouh crypto list and pull based on start/end date\n",
    "for crypto in cryptolist:\n",
    "    fetch_main_cryptos(crypto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created a path to our newly created CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "BTC_path, ETH_path, LTC_path = (Path('../Justin_edits/Coinbase_BTCUSD_dailydata.csv'),\n",
    "                                Path('../Justin_edits/Coinbase_ETHUSD_dailydata.csv'),\n",
    "                                Path('../Justin_edits/Coinbase_LTCUSD_dailydata.csv'))\n",
    "BTC_df, ETH_df, LTC_df = (pd.read_csv(BTC_path, index_col='date', infer_datetime_format=False, parse_dates=True),\n",
    "                          pd.read_csv(ETH_path, index_col='date', infer_datetime_format=False, parse_dates=True),\n",
    "                          pd.read_csv(LTC_path, index_col='date', infer_datetime_format=False, parse_dates=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is to create our main datasets. Please edit and comment on how we should approach this.\n",
    "#user, category, date, polarity, popularity, fav_count, volume, \n",
    "def clean_data(df):\n",
    "    df = df.dropna() # immediately drop any null values\n",
    "    df = df.drop(columns=['unix']).copy() #create deep copy of df with desired columns\n",
    "    df['volume_change'] = round(df['volume'].pct_change(), 2) #find daily percent change in volume\n",
    "    df['percent_volatility'] = round(((df['high'] - df['low']) / df['high']) * 100, 2) #Finding the amount of change between the low and high, then comparing it to the high.\n",
    "    df['daily_change'] = round(df['close'].pct_change(), 5) # daily pct change\n",
    "    df['daily_avg_price'] = round((df['open'] + df['close']) / 2, 2)\n",
    "    df['rolling_volatility'] = round(df['percent_volatility'].rolling(30).mean(), 2)\n",
    "    df['avg_volume_change'] = df['volume_change'].rolling(30).mean()\n",
    "    df['rolling_volume'] = round(df['volume'].rolling(3).mean(), 2)\n",
    "    df.drop(df.head(2).index, inplace=True) # drop the unfinished and upcoming day, inclusive of NA data\n",
    "    df.drop(columns=['low', 'close', 'open'], inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    return pd.DataFrame(df)\n",
    "def clean_new_data(df):\n",
    "    return df.dropna()\n",
    "#these are the base data sets so far\n",
    "BTC_df = clean_data(BTC_df).dropna()\n",
    "ETH_df = clean_data(ETH_df)\n",
    "LTC_df = clean_data(LTC_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new data frames to compare statistical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined columns into new dataframe and renamed\n",
    "BTC_volume = BTC_df['volume']\n",
    "ETH_volume = ETH_df['volume']\n",
    "LTC_volume = LTC_df['volume']\n",
    "volume_df = clean_new_data(pd.concat([BTC_volume, ETH_volume, LTC_volume], axis=1))\n",
    "volume_df.columns = ['BTC_volume', 'ETH_volume', 'LTC_volume']\n",
    "\n",
    "#joined columns into new dataframe and renamed\n",
    "BTC_rolling_volume = BTC_df['rolling_volume']\n",
    "ETH_rolling_volume = ETH_df['rolling_volume']\n",
    "LTC_rolling_volume = LTC_df['rolling_volume']\n",
    "rolling_volume_change_df = clean_new_data(pd.concat([BTC_rolling_volume, ETH_rolling_volume, LTC_rolling_volume], axis=1))\n",
    "rolling_volume_change_df.columns = ['BTC_rolling_volume', 'ETH_rolling_volume', 'LTC_rolling_volume']\n",
    "\n",
    "\n",
    "#joined columns into new dataframe and renamed\n",
    "BTC_volume_change = BTC_df['volume_change']\n",
    "ETH_volume_change = ETH_df['volume_change']\n",
    "LTC_volume_change = LTC_df['volume_change']\n",
    "volume_change_df = clean_new_data(pd.concat([BTC_volume_change, ETH_volume_change, LTC_volume_change], axis=1))\n",
    "volume_change_df.columns = ['BTC_volume_change', 'ETH_volume_change', 'LTC_volume_change']\n",
    "\n",
    "#joined columns into new dataframe and renamed\n",
    "BTC_volatilityr = BTC_df['rolling_volatility']\n",
    "ETH_volatilityr = ETH_df['rolling_volatility']\n",
    "LTC_volatilityr = LTC_df['rolling_volatility']\n",
    "volatilityr_df = clean_new_data(pd.concat([BTC_volatilityr, ETH_volatilityr, LTC_volatilityr], axis=1))\n",
    "volatilityr_df.columns = ['BTC_rolling_volatility', 'ETH_rolling_volatility', 'LTC_rolling_volatility']\n",
    "\n",
    "#joined columns into new dataframe and renamed\n",
    "BTC_volatility = BTC_df['percent_volatility']\n",
    "ETH_volatility = ETH_df['percent_volatility']\n",
    "LTC_volatility = LTC_df['percent_volatility']\n",
    "volatility_df = clean_new_data(pd.concat([BTC_volatility, ETH_volatility, LTC_volatility], axis=1))\n",
    "volatility_df.columns = ['BTC_volatility', 'ETH_volatility', 'LTC_volatility']\n",
    "\n",
    "#joined columns into new dataframe and renamed\n",
    "BTC_high = BTC_df['high']\n",
    "ETH_high = ETH_df['high']\n",
    "LTC_high = LTC_df['high']\n",
    "close_df = clean_new_data(pd.concat([BTC_high, ETH_high, LTC_high], axis=1))\n",
    "close_df.columns = ['BTC_high', 'ETH_high', 'LTC_high']\n",
    "ETH_LTC_close_df = clean_new_data(close_df.drop(columns='BTC_high'))\n",
    "\n",
    "#joined columns into new dataframe and renamed\n",
    "BTC_daily_change = BTC_df['daily_change']\n",
    "ETH_daily_change = ETH_df['daily_change']\n",
    "LTC_daily_change = LTC_df['daily_change']\n",
    "daily_change_df = clean_new_data(pd.concat([BTC_daily_change, ETH_daily_change, LTC_daily_change], axis=1))\n",
    "daily_change_df.columns = ['BTC_daily_change', 'ETH_daily_change', 'LTC_daily_change']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull twitter data, then clean and create merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_path = Path('../data/raw_data/raw_tweets_01_filter_polarity.csv')\n",
    "twitter_df = pd.read_csv(twitter_path, infer_datetime_format=True, parse_dates=True)\n",
    "datetime = twitter_df['time'].str.split(\" \", n=1, expand = True)\n",
    "twitter_df['date'] = datetime[0]\n",
    "twitter_df.drop(columns='time')\n",
    "twitter_df['time'] = datetime[1]\n",
    "twitter_df.drop_duplicates(subset='text', inplace=True)\n",
    "twitter_df.set_index('date', inplace=True)\n",
    "#TODO: round all columns\n",
    "df = pd.merge(twitter_df, volume_change_df, how='inner', left_index=True, right_index=True)\n",
    "df2 = pd.merge(df, volatility_df, how='inner', left_index=True, right_index=True)\n",
    "df3 = pd.merge(df2, close_df, how='inner', left_index=True, right_index=True)\n",
    "df4 = pd.merge(df3, volatilityr_df, how='inner', left_index=True, right_index=True)\n",
    "df5 = pd.merge(df4, rolling_volume_change_df, how='inner', left_index=True, right_index=True)\n",
    "df6 = pd.merge(df5, volume_df, how='inner', left_index=True, right_index=True)\n",
    "twitter_analysis_df = pd.merge(df6, daily_change_df, how='inner', left_index=True, right_index=True)\n",
    "twitter_analysis_df['Popularity Rating'] = round(twitter_analysis_df['Popularity Rating'], 2)\n",
    "twitter_analysis_df.sort_index(inplace=True)\n",
    "twitter_analysis_df['category'].fillna('null', inplace=True)\n",
    "\n",
    "#tweet_analysis_df['2020-03-30':'2021-19-01']\n",
    "#df1.merge(df2, on='ID', how='left')\n",
    "def export_df(df):\n",
    "    return df.to_csv('../data/raw_data/raw_crypto_data.csv')\n",
    "export_df(twitter_analysis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is some example data. We can add any tables and mess with the data from here. Anything we want to add on we should do through our crypto data frames and restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['VitalikButerin', 'officialmcafee', 'joerogan', 'jack',\n",
       "       'SatoshiLite', 'elonmusk'], dtype=object)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_users = twitter_analysis_df['twitter_user'].unique()\n",
    "crypto_category = twitter_analysis_df['category'].unique()\n",
    "crypto_category\n",
    "bitcoin_tweets_df = twitter_analysis_df[twitter_analysis_df['category'].str.contains('bit')]\n",
    "ethereum_tweets_df = twitter_analysis_df[twitter_analysis_df['category'].str.contains('eth')]\n",
    "twitter_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = twitter_analysis_df[twitter_analysis_df['category'] == 'bitcoin']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miche\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3990: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "elon = twitter_users[5]\n",
    "elon_tweets_df = twitter_analysis_df[twitter_analysis_df['twitter_user'] == elon]\n",
    "elon_tweets1 = elon_tweets_df[elon_tweets_df['Popularity Rating'] != 0]\n",
    "elon_tweets_rated = elon_tweets1[elon_tweets1['category'] == 'bitcoin']\n",
    "elon_tweets_rated.drop(columns=['sentiment', 'time', 'tweet_id', 'tweet_source', 'quote_count', 'reply_count', 'retweet_count'], inplace=True)\n",
    "def export_elon_data(df):\n",
    "    return df.to_csv('../data/clean_data/clean_elon_data.csv')\n",
    "export_elon_data(elon_tweets_rated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
